{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Joining, Filtering, and Loading Relational Data with AWS Glue\n\nThis example shows how to do joins and filters with transforms entirely on DynamicFrames.\n\n### 1. Crawl our sample dataset\n\nThe dataset we'll be using in this example was downloaded from the [EveryPolitician](http://everypolitician.org)\nwebsite into our sample-dataset bucket in S3, at:\n\n    s3://awsglue-datasets/examples/us-legislators.\n\nIt contains data in JSON format about United States legislators and the seats they have held\nin the the House of Representatives and the Senate.\n\nFor purposes of our example code, we are assuming that you have created an AWS S3 target bucket and folder,\nwhich we refer to here as `s3://glue-sample-target/output-dir/`.\n\nThe first step is to crawl this data and put the results into a database called `legislators`\nin your Data Catalog, as described [here in the Developer Guide](http://docs.aws.amazon.com/glue/latest/dg/console-crawlers.html).\nThe crawler will create the following tables in the `legislators` database:\n\n - `persons_json`\n - `memberships_json`\n - `organizations_json`\n - `events_json`\n - `areas_json`\n - `countries_r_json`\n\nThis is a semi-normalized collection of tables containing legislators and their histories.\n\n### 2. Getting started\n\nWe will write a script that:\n\n1. Combines persons, organizations, and membership histories into a single legislator\n   history data set. This is often referred to as de-normalization.\n2. Separates out the senators from the representatives.\n3. Writes each of these out to separate parquet files for later analysis.\n\nBegin by running some boilerplate to import the AWS Glue libraries we'll need and set up a single `GlueContext`.\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n## TO BE REPLACED BY YOUR BUCKET NAME ##\nmy_output_bucket_name = \"<YOUR BUCKET NAME> (eg. demo-legislators-293278930)\"\n\nglueContext = GlueContext(SparkContext.getOrCreate())",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "import boto3\n\nclient = boto3.client('glue', 'eu-west-1')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0 and 3.0. \n                                      Default: 2.0.\n----\n\n## Selecting Job Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n----\n\n## Glue Config Magic \n*(common across all job types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n----\n\n                                      \n## Magic for Spark Jobs (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n                                      ETL and Streaming support G.1X and G.2X. \n                                      Default: G.1X.\n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Job\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray job. \n                                      Default: 0.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### 3. Checking the schemas that the crawler identified\n\nNext, you can easily examine the schemas that the crawler recorded in the Data Catalog. For example,\nto see the schema of the `persons_json` table, run the following code:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "persons = glueContext.create_dynamic_frame.from_catalog(database=\"legislators\", table_name=\"persons_json\")\nprint(\"Count: \" + str(persons.count()))\npersons.printSchema()",
			"metadata": {
				"scrolled": true,
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "Count: 1961\nroot\n|-- family_name: string\n|-- name: string\n|-- links: array\n|    |-- element: struct\n|    |    |-- note: string\n|    |    |-- url: string\n|-- gender: string\n|-- image: string\n|-- identifiers: array\n|    |-- element: struct\n|    |    |-- scheme: string\n|    |    |-- identifier: string\n|-- other_names: array\n|    |-- element: struct\n|    |    |-- lang: string\n|    |    |-- note: string\n|    |    |-- name: string\n|-- sort_name: string\n|-- images: array\n|    |-- element: struct\n|    |    |-- url: string\n|-- given_name: string\n|-- birth_date: string\n|-- id: string\n|-- contact_details: array\n|    |-- element: struct\n|    |    |-- type: string\n|    |    |-- value: string\n|-- death_date: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Each person in the table is a member of some congressional body.\n\nLook at the schema of the `memberships_json` table:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "memberships = glueContext.create_dynamic_frame.from_catalog(database=\"legislators\", table_name=\"memberships_json\")\nprint(\"Count: \" + str(memberships.count()))\nmemberships.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Count: 10439\nroot\n|-- area_id: string\n|-- on_behalf_of_id: string\n|-- organization_id: string\n|-- role: string\n|-- person_id: string\n|-- legislative_period_id: string\n|-- start_date: string\n|-- end_date: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Organizations are parties and the two chambers of congress, the Senate and House.\nLook at the schema of the `organizations_json` table:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "orgs = glueContext.create_dynamic_frame.from_catalog(database=\"legislators\", table_name=\"organizations_json\")\nprint(\"Count: \" + str(orgs.count()))\norgs.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Count: 13\nroot\n|-- identifiers: array\n|    |-- element: struct\n|    |    |-- scheme: string\n|    |    |-- identifier: string\n|-- other_names: array\n|    |-- element: struct\n|    |    |-- lang: string\n|    |    |-- note: string\n|    |    |-- name: string\n|-- id: string\n|-- classification: string\n|-- name: string\n|-- links: array\n|    |-- element: struct\n|    |    |-- note: string\n|    |    |-- url: string\n|-- image: string\n|-- seats: int\n|-- type: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### 4. Filtering\n\nLet's only keep the fields that we want and rename `id` to `org_id`. The dataset is small enough that we can\nlook at the whole thing. The `toDF()` converts a DynamicFrame to a Spark DataFrame, so we can apply the\ntransforms that already exist in SparkSQL:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "orgs = orgs.drop_fields(['other_names','identifiers']).rename_field('id', 'org_id').rename_field('name', 'org_name')\norgs.toDF().show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------+--------------------+--------------------+--------------------+--------------------+-----+-----------+\n|classification|              org_id|            org_name|               links|               image|seats|       type|\n+--------------+--------------------+--------------------+--------------------+--------------------+-----+-----------+\n|         party|            party/al|                  AL|                null|                null| null|       null|\n|         party|      party/democrat|            Democrat|[[website, http:/...|https://upload.wi...| null|       null|\n|         party|party/democrat-li...|    Democrat-Liberal|[[website, http:/...|                null| null|       null|\n|   legislature|d56acebe-8fdc-47b...|House of Represen...|                null|                null|  435|lower house|\n|         party|   party/independent|         Independent|                null|                null| null|       null|\n|         party|party/new_progres...|     New Progressive|[[website, http:/...|https://upload.wi...| null|       null|\n|         party|party/popular_dem...|    Popular Democrat|[[website, http:/...|                null| null|       null|\n|         party|    party/republican|          Republican|[[website, http:/...|https://upload.wi...| null|       null|\n|         party|party/republican-...|Republican-Conser...|[[website, http:/...|                null| null|       null|\n|         party|      party/democrat|            Democrat|[[website, http:/...|https://upload.wi...| null|       null|\n|         party|   party/independent|         Independent|                null|                null| null|       null|\n|         party|    party/republican|          Republican|[[website, http:/...|https://upload.wi...| null|       null|\n|   legislature|8fa6c3d2-71dc-478...|              Senate|                null|                null|  100|upper house|\n+--------------+--------------------+--------------------+--------------------+--------------------+-----+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Let's look at the `organizations` that appear in `memberships`:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "memberships.select_fields(['organization_id']).toDF().distinct().show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+\n|     organization_id|\n+--------------------+\n|8fa6c3d2-71dc-478...|\n|d56acebe-8fdc-47b...|\n+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### 5. Putting it together\n\nNow let's join these relational tables to create one full history table of legislator\nmemberships and their correponding organizations, using AWS Glue.\n\n - First, we join `persons` and `memberships` on `id` and `person_id`.\n - Next, join the result with orgs on `org_id` and `organization_id`.\n - Then, drop the redundant fields, `person_id` and `org_id`.\n\nWe can do all these operations in one (extended) line of code:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "l_history = Join.apply(orgs,\n                       Join.apply(persons, memberships, 'id', 'person_id'),\n                       'org_id', 'organization_id').drop_fields(['person_id', 'org_id'])\nprint(\"Count: \" + str(l_history.count()))\nl_history.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "Execution Interrupted. Attempting to cancel the statement (statement_id=7)\nStatement 7 has been cancelled\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Great! We now have the final table that we'd like to use for analysis.\nLet's write it out in a compact, efficient format for analytics, i.e. Parquet,\nthat we can run SQL over in AWS Glue, Athena, or Redshift Spectrum.\n\nThe following call writes the table across multiple files to support fast parallel\nreads when doing analysis later:\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "glueContext.write_dynamic_frame.from_options(frame = l_history,\n              connection_type = \"s3\",\n              connection_options = {\"path\": \"s3://\" + my_output_bucket_name + \"/output-dir/legislator_history\"},\n              format = \"parquet\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f5a64ad3d10>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "To put all the history data into a single file, we need to convert it to a data frame, repartition it, and\nwrite it out.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "s_history = l_history.toDF().repartition(1)\ns_history.write.mode(\"overwrite\").parquet('s3://'+ my_output_bucket_name +'/output-dir/legislator_single')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Or if you want to separate it by the Senate and the House:\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "l_history.toDF().write.mode(\"overwrite\").parquet('s3://'+ my_output_bucket_name +'/output-dir/legislator_part', partitionBy=['org_name'])",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}